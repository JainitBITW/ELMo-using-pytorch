{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import cuda\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the CONSTANTS \n",
    "\n",
    "EXCLUDE_STOPWORDS = True\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 100\n",
    "GLOVE_PATH = 'glove/glove.6B.100d.txt'\n",
    "DEVICE = 'cuda'\n",
    "if cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/turning/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 154.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove[line[0]] = torch.tensor([float(x) for x in line[1:]])\n",
    "\n",
    "# create a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "glove['<unk>'] = torch.mean(torch.stack(list(glove.values())), dim=0)\n",
    "glove['<pad>'] = torch.zeros(EMBEDDING_DIM)\n",
    "glove['<start>'] = torch.rand(EMBEDDING_DIM)\n",
    "glove['<end>'] = torch.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the word_2_idx and idx_2_word dictionaries and the embedding matrix\n",
    "word_2_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
    "idx_2_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
    "embedding_matrix = np.zeros((len(glove.values()), EMBEDDING_DIM))\n",
    "embedding_matrix[0] = glove['<pad>']\n",
    "embedding_matrix[1] = glove['<unk>']\n",
    "embedding_matrix[2] = glove['<start>']\n",
    "embedding_matrix[3] = glove['<end>']\n",
    "\n",
    "for i, word in enumerate(glove.keys()):\n",
    "    if word not in word_2_idx:\n",
    "        word_2_idx[word] = len(word_2_idx)\n",
    "        idx_2_word[len(idx_2_word)] = word\n",
    "        embedding_matrix[word_2_idx[word]] = glove[word]\n",
    "\n",
    "# convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ready the datasets : \n",
    "\n",
    "raw_datasets = {'train': [], 'validation':[], 'test': []}\n",
    "\n",
    "for i in dataset:\n",
    "    for j in dataset[i]:\n",
    "        if EXCLUDE_STOPWORDS:\n",
    "            tokens = [word.lower() for word in j['tokens'].split('|') if word.lower() not in stop_words]\n",
    "        else:\n",
    "            tokens = [word.lower() for word in j['tokens'].split('|')]\n",
    "        numbered_tokens = [word_2_idx[word] if word in word_2_idx else word_2_idx['<unk>'] for word in tokens]\n",
    "\n",
    "        raw_datasets[i].append([numbered_tokens, j['label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pretrain = {'train': [], 'validation':[], 'test': []}\n",
    "dataset_sem_anl= {'train': [], 'validation':[], 'test': []}\n",
    "\n",
    "for i in raw_datasets:\n",
    "    for j in raw_datasets[i]:\n",
    "        j[0] = [word_2_idx['<start>']] + j[0] + [word_2_idx['<end>']]\n",
    "        j[0]= torch.LongTensor(j[0])\n",
    "        dataset_pretrain[i].append({'sentence': j[0], 'label': j[0][:-1]})\n",
    "        dataset_sem_anl[i].append({'sentence': j[0], 'label': j[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    2,  1141, 10457,  5037,   593,    13,    54,    32, 18516,    31,\n",
      "           13,   226,   163, 16810,   155,  1417,  5822,  6684,     5,     1,\n",
      "         1465, 43711,  4415, 26988,     6,     3]), tensor([    2,  1141, 10457,  5037,   593,    13,    54,    32, 18516,    31,\n",
      "           13,   226,   163, 16810,   155,  1417,  5822,  6684,     5,     1,\n",
      "         1465, 43711,  4415, 26988,     6]))\n"
     ]
    }
   ],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['sentence'], self.data[idx]['label']\n",
    "    \n",
    "pretrain_dataset = {'train': PretrainDataset(dataset_pretrain['train']), 'validation': PretrainDataset(dataset_pretrain['validation']), 'test': PretrainDataset(dataset_pretrain['test'])}\n",
    "sman_dataset = {'train': PretrainDataset(dataset_sem_anl['train']), 'validation': PretrainDataset(dataset_sem_anl['validation']), 'test': PretrainDataset(dataset_sem_anl['test'])}\n",
    "\n",
    "print(pretrain_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    sentences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True)\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return  padded_sentences,padded_labels\n",
    "\n",
    "def custom_collate_sman(batch):\n",
    "    sentences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    for i , label in enumerate(labels):\n",
    "        if label <=0.5:\n",
    "            labels[i]=0\n",
    "        else :\n",
    "            labels[i]=1\n",
    "\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True)\n",
    "    \n",
    "    return  padded_sentences,labels\n",
    "\n",
    "pretrain_loaders={}\n",
    "sman_loaders={}\n",
    "for i in pretrain_dataset:\n",
    "    pretrain_loaders[i] = DataLoader(pretrain_dataset[i], batch_size=BATCH_SIZE, collate_fn=custom_collate)\n",
    "    sman_loaders[i] = DataLoader(sman_dataset[i], batch_size=BATCH_SIZE, collate_fn=custom_collate_sman)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the model which we are going to pretrain\n",
    "class ELMo(nn.Module):\n",
    "    '''this class implements the ELMo model using the BI-LSTM architecture like by stacking two LSTM layers \n",
    "    the model is just the head and needs body such as linear layer , mlp , etc based on the task  '''\n",
    "    def __init__(self, embedding_dim,  hidden_dim1, hidden_dim2 ,batch_size, num_layers=2):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding= nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.weight1 = nn.Parameter(torch.randn(1))\n",
    "        self.weight2 = nn.Parameter(torch.randn(1))\n",
    "        self.lambda1 = nn.Parameter(torch.randn(1))\n",
    "\n",
    "\n",
    "    def forward(self, input): \n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        input_embeddings = self.embedding(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # passing the embeddings to the first LSTM layer\n",
    "        output1 , (hidden1, cell1) = self.lstm1(input_embeddings) # [batch_size, seq_len, hidden_dim1]\n",
    "\n",
    "        # passing the output of the first LSTM layer to the second LSTM layer\n",
    "        output2 , (hidden2, cell2) = self.lstm2(output1) # [batch_size, seq_len, hidden_dim2]\n",
    "        # adding the two outputs of the LSTM layers\n",
    "        \n",
    "        weighted_output = self.lambda1*((self.weight1 * output1) +( self.weight2 * output2))\n",
    "\n",
    "        return weighted_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_model(nn.Module):\n",
    "    '''this class implements the language model using the ELMo model as the head and a linear layer as the body'''\n",
    "    def __init__(self, Elmo_model, vocab_size, embedding_dim):\n",
    "        super(Language_model, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.elmo = Elmo_model\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.vocab_size)\n",
    "    def forward(self, input):\n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        elmo_output = self.elmo(input) # [batch_size, seq_len, embedding_dim]\n",
    "        output = self.linear(elmo_output) # [batch_size, seq_len, vocab_size]\n",
    "        output = F.log_softmax(output, dim=2).permute(0,2,1)[:,:,:-1] # [batch_size, vocab_size, seq_len-1]\n",
    "        return output\n",
    "    \n",
    "class Semantic_analysis(nn.Module):\n",
    "    '''this class implements the semantic analysis model using the ELMo model as the head and a linear layer as the body'''\n",
    "    def __init__(self, Elmo_model, vocab_size, embedding_dim, num_classes=2):\n",
    "        super(Semantic_analysis, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo = Elmo_model\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        elmo_output = self.elmo(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # get length of each sentence \n",
    "        lengths = []\n",
    "        for i in input : \n",
    "            flag=1\n",
    "            for n , j in enumerate(i):\n",
    "                if j==0:\n",
    "                    lengths.append(n)\n",
    "                    flag =0 \n",
    "                    break\n",
    "            if flag : \n",
    "                lengths.append(len(i))\n",
    "        # get the sentence embedding by taking the mean of the word embeddings\n",
    "        sentence_embeddings = []\n",
    "        for i in range(len(elmo_output)):\n",
    "            sentence_embeddings.append(torch.mean(elmo_output[i][:lengths[i]], dim=0))\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings)  # convert list to tensor\n",
    "\n",
    "        \n",
    "        output = self.linear(sentence_embeddings) # [batch_size, num_classes]\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ELMo(embedding_dim=EMBEDDING_DIM, hidden_dim1=EMBEDDING_DIM//2, hidden_dim2=EMBEDDING_DIM//2, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lm= Language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Language_model(elmo, vocab_size=len(glove), embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# define the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = 1000000\n",
    "best_accuracy = 0\n",
    "def accuracy(output, label):\n",
    "    output = output.argmax(dim=1)\n",
    "    return (output == label).float().mean()\n",
    "steps = 0\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    print('epoch: ', epoch)\n",
    "    if epoch%3 == 0 and epoch != 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    for input, label in pretrain_loaders['train']:\n",
    "        steps += 1\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        input = input.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = model.forward(input)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps%15 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_accuracy = 0\n",
    "                for input, label in pretrain_loaders['validation']:\n",
    "                    input = input.to(DEVICE)\n",
    "                    label = label.to(DEVICE)\n",
    "                    output = model.forward(input)\n",
    "                    val_loss += criterion(output, label)\n",
    "                    val_accuracy += accuracy(output, label)\n",
    "                val_loss = val_loss/len(pretrain_loaders['validation'])\n",
    "                val_accuracy = val_accuracy/len(pretrain_loaders['validation'])\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(model.state_dict(), 'best_loss.pth')\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    torch.save(model.state_dict(), 'best_accuracy.pth')\n",
    "                print( 'train loss: ', running_loss/100, 'validation loss: ', val_loss, 'validation accuracy: ', val_accuracy)\n",
    "                running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = Semantic_analysis(elmo, vocab_size=len(glove), embedding_dim=EMBEDDING_DIM, num_classes=1)\n",
    "\n",
    "for param in elmo.parameters():\n",
    "    param.requires_grad = False\n",
    "elmo.weight1.requires_grad = True\n",
    "elmo.weight2.requires_grad = True\n",
    "elmo.lambda1.requires_grad = True\n",
    "for params in sem.parameters():\n",
    "    print(params.requires_grad)\n",
    "def vaccuracy(output, target, threshold=0.5):\n",
    "    \"\"\"Computes accuracy for float values between 0 and 1\"\"\"\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        predicted = (output >= threshold).float()\n",
    "        target = ( target >= threshold).float()\n",
    "        correct = (predicted == target).sum().item()\n",
    "        total = target.size(0)\n",
    "        acc = correct / total\n",
    "    return acc\n",
    "sem.to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(sem.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = 1000000\n",
    "best_accuracy = 0\n",
    "steps = 0\n",
    "EPOCHS= 3\n",
    "running_loss = 0\n",
    "for e in range(EPOCHS):\n",
    "    print('epoch: ', e)\n",
    "    if e%3 == 0 and e != 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    for input, label in sman_loaders['train']:\n",
    "        steps += 1\n",
    "        optimizer.zero_grad()\n",
    "        sem.zero_grad()\n",
    "        input = input.to(DEVICE)\n",
    "\n",
    "        label = label.to(DEVICE)\n",
    "        output = sem.forward(input).squeeze(dim=1)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps%15 == 0:\n",
    "            sem.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_accuracy = 0\n",
    "                for input, label in sman_loaders['validation']:\n",
    "                    input = input.to(DEVICE)\n",
    "                \n",
    "                    label = label.to(DEVICE)\n",
    "                    output = sem.forward(input).squeeze(dim=1)\n",
    "                   \n",
    "                    val_loss += criterion(output, label)\n",
    "                    val_accuracy+= vaccuracy(output , label)\n",
    "        \n",
    "                val_loss = val_loss/len(sman_loaders['validation'])\n",
    "                val_accuracy = val_accuracy/len(sman_loaders['validation'])\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(sem.state_dict(), 'bl.pth')\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    torch.save(sem.state_dict(), 'ba.pth')\n",
    "                print( 'train loss: ', running_loss/100, 'validation loss: ', val_loss, 'validation accuracy: ', val_accuracy)\n",
    "                running_loss = 0\n",
    "            sem.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
