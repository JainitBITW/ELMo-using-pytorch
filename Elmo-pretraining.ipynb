{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turning/anaconda3/envs/torchy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/turning/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/turning/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the CONSTANTS \n",
    "\n",
    "INCLUDE_STOPWORDS = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 100\n",
    "GLOVE_PATH = 'glove/glove.6B.100d.txt'\n",
    "DEVICE = 'cuda'\n",
    "if cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/turning/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 931.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset and loading the glove embeddings \n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove[line[0]] = torch.tensor([float(x) for x in line[1:]])\n",
    "\n",
    "# create a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "glove['<unk>'] = torch.mean(torch.stack(list(glove.values())), dim=0)\n",
    "glove['<pad>'] = torch.zeros(EMBEDDING_DIM)\n",
    "glove['<start>'] = torch.rand(EMBEDDING_DIM)\n",
    "glove['<end>'] = torch.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the word_2_idx and idx_2_word dictionaries and the embedding matrix\n",
    "word_2_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
    "idx_2_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
    "embedding_matrix = np.zeros((len(glove.values()), EMBEDDING_DIM))\n",
    "embedding_matrix[0] = glove['<pad>']\n",
    "embedding_matrix[1] = glove['<unk>']\n",
    "embedding_matrix[2] = glove['<start>']\n",
    "embedding_matrix[3] = glove['<end>']\n",
    "\n",
    "for i, word in enumerate(glove.keys()):\n",
    "    if word not in word_2_idx:\n",
    "        word_2_idx[word] = len(word_2_idx)\n",
    "        idx_2_word[len(idx_2_word)] = word\n",
    "        embedding_matrix[word_2_idx[word]] = glove[word]\n",
    "\n",
    "# convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the model which we are going to pretrain\n",
    "class ELMo(nn.Module):\n",
    "    '''this class implements the ELMo model using the BI-LSTM architecture like by stacking two LSTM layers'''\n",
    "    def __init__(self, embedding_dim, vocab_size,  hidden_dim1, hidden_dim2 ,batch_size, num_layers=2):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.vocb_size =  vocab_size\n",
    "        self.embedding= nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim2*2, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input): \n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        input_embeddings = self.embedding(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # passing the embeddings to the first LSTM layer\n",
    "        self.output1 , (hidden1, cell1) = self.lstm1(input_embeddings) # [batch_size, seq_len, hidden_dim1]\n",
    "\n",
    "        # passing the output of the first LSTM layer to the second LSTM layer\n",
    "        self.output2 , (hidden2, cell2) = self.lstm2(self.output1) # [batch_size, seq_len, hidden_dim2]\n",
    "        # adding the two outputs of the LSTM layers\n",
    "    \n",
    "        # output = [batch_size, seq_len, vocab_size]\n",
    "        output = self.linear(self.output2)\n",
    "        output_softmax = F.log_softmax(output, dim=2)\n",
    "        # removing the last token from the output as we are pretraing the model \n",
    "        output_softmax = output_softmax.permute(0,2,1)[:,:,:-1]\n",
    "\n",
    "        return output_softmax\n",
    "    \n",
    "    def get_weighted_outputs(self, input):\n",
    "        '''this function returns the weighted outputs of the two LSTM layers'''\n",
    "\n",
    "        # getting the output embeddings also freezing the parameters of the  lstm layers\n",
    "        with torch.no_grad():\n",
    "            input_embeddings = self.embedding(input)\n",
    "            output1 , (hidden1, cell1) = self.lstm1(input_embeddings)\n",
    "            output2 , (hidden2, cell2) = self.lstm2(output1)\n",
    "        # getting the weights for the weighted sum of the two outputs \n",
    "        weight1 = nn.Parameter(torch.randn(1))\n",
    "        weight2 = nn.Parameter(torch.randn(1))\n",
    "        lambda1 = nn.Parameter(torch.randn(1))\n",
    "        weighted_output = lambda1(weight1 * output1 + weight2 * output2)\n",
    "\n",
    "        return weighted_output        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
