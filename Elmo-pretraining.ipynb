{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/turning/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/turning/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the CONSTANTS \n",
    "\n",
    "EXCLUDE_STOPWORDS = True\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 100\n",
    "GLOVE_PATH = 'glove/glove.6B.100d.txt'\n",
    "DEVICE = 'cuda'\n",
    "if cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/turning/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1461.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset and loading the glove embeddings \n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove[line[0]] = torch.tensor([float(x) for x in line[1:]])\n",
    "\n",
    "# create a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "glove['<unk>'] = torch.mean(torch.stack(list(glove.values())), dim=0)\n",
    "glove['<pad>'] = torch.zeros(EMBEDDING_DIM)\n",
    "glove['<start>'] = torch.rand(EMBEDDING_DIM)\n",
    "glove['<end>'] = torch.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the word_2_idx and idx_2_word dictionaries and the embedding matrix\n",
    "word_2_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
    "idx_2_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
    "embedding_matrix = np.zeros((len(glove.values()), EMBEDDING_DIM))\n",
    "embedding_matrix[0] = glove['<pad>']\n",
    "embedding_matrix[1] = glove['<unk>']\n",
    "embedding_matrix[2] = glove['<start>']\n",
    "embedding_matrix[3] = glove['<end>']\n",
    "\n",
    "for i, word in enumerate(glove.keys()):\n",
    "    if word not in word_2_idx:\n",
    "        word_2_idx[word] = len(word_2_idx)\n",
    "        idx_2_word[len(idx_2_word)] = word\n",
    "        embedding_matrix[word_2_idx[word]] = glove[word]\n",
    "\n",
    "# convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the model which we are going to pretrain\n",
    "class ELMo(nn.Module):\n",
    "    '''this class implements the ELMo model using the BI-LSTM architecture like by stacking two LSTM layers'''\n",
    "    def __init__(self, embedding_dim, vocab_size,  hidden_dim1, hidden_dim2 ,batch_size, num_layers=2):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.vocb_size =  vocab_size\n",
    "        self.embedding= nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim2*2, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input): \n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        input_embeddings = self.embedding(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # passing the embeddings to the first LSTM layer\n",
    "        self.output1 , (hidden1, cell1) = self.lstm1(input_embeddings) # [batch_size, seq_len, hidden_dim1]\n",
    "\n",
    "        # passing the output of the first LSTM layer to the second LSTM layer\n",
    "        self.output2 , (hidden2, cell2) = self.lstm2(self.output1) # [batch_size, seq_len, hidden_dim2]\n",
    "        # adding the two outputs of the LSTM layers\n",
    "    \n",
    "        # output = [batch_size, seq_len, vocab_size]\n",
    "        output = self.linear(self.output2)\n",
    "        output_softmax = F.log_softmax(output, dim=2)\n",
    "        # removing the last token from the output as we are pretraing the model \n",
    "        output_softmax = output_softmax.permute(0,2,1)[:,:,:-1]\n",
    "\n",
    "        return output_softmax\n",
    "    \n",
    "    def get_weighted_outputs(self, input):\n",
    "        '''this function returns the weighted outputs of the two LSTM layers'''\n",
    "\n",
    "        # getting the output embeddings also freezing the parameters of the  lstm layers\n",
    "        with torch.no_grad():\n",
    "            input_embeddings = self.embedding(input)\n",
    "            output1 , (hidden1, cell1) = self.lstm1(input_embeddings)\n",
    "            output2 , (hidden2, cell2) = self.lstm2(output1)\n",
    "        # getting the weights for the weighted sum of the two outputs \n",
    "        weight1 = nn.Parameter(torch.randn(1))\n",
    "        weight2 = nn.Parameter(torch.randn(1))\n",
    "        lambda1 = nn.Parameter(torch.randn(1))\n",
    "        weighted_output = lambda1(weight1 * output1 + weight2 * output2)\n",
    "\n",
    "        return weighted_output        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the datasets like tokenising them \n",
    "prediction_raw_datasets={}\n",
    "prediction_raw_datasets['train'] = [ i.lower().split('|') for i in dataset['train']['tokens']]\n",
    "prediction_raw_datasets['validation'] = [ i.lower().split('|') for i in dataset['validation']['tokens']]\n",
    "prediction_raw_datasets['test'] = [ i.lower().split('|') for i in dataset['test']['tokens']]\n",
    "\n",
    "for k , v in prediction_raw_datasets.items():\n",
    "    for i in range(len(v)):\n",
    "        if EXCLUDE_STOPWORDS:\n",
    "            v[i] = [word for word in v[i] if word not in stop_words]\n",
    "        for j in range(len(v[i])):\n",
    "            if v[i][j] not in word_2_idx:\n",
    "                v[i][j] = '<unk>'\n",
    "\n",
    "        v[i]= ['<start>'] + v[i] + ['<end>']\n",
    "        v[i] = [word_2_idx[word] for word in v[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the datasets with sentence and label\n",
    "datasets = {'train': [], 'validation': [], 'test': []}\n",
    "for i in range(len(prediction_raw_datasets['train'])):  \n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['train'][i])                                        \n",
    "    datasets['train'].append({'sentence': sentence, 'label': sentence[:-1]})\n",
    "for i in range(len(prediction_raw_datasets['validation'])):  \n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['validation'][i])                                        \n",
    "    datasets['validation'].append({'sentence': sentence, 'label': sentence[1:]})\n",
    "for i in range(len(prediction_raw_datasets['test'])):\n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['test'][i])                                        \n",
    "    datasets['test'].append({'sentence': sentence, 'label': sentence[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig the obejct model\n",
    "model = ELMo(EMBEDDING_DIM, len(glove), HIDDEN_DIM, EMBEDDING_DIM//2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  12.484819412231445\n",
      "accuracy:  tensor(0.0800)\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     27\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# define the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = 1000000\n",
    "best_accuracy = 0\n",
    "def accuracy(output, label):\n",
    "    output = output.argmax(dim=1)\n",
    "    return (output == label).float().mean()\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('epoch: ', epoch)\n",
    "    if epoch%3 == 0 and epoch != 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    for i in range(len(datasets['test'])):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        input = datasets['train'][i]['sentence'].unsqueeze(0)\n",
    "        label = datasets['train'][i]['label'].unsqueeze(0)\n",
    "        input = input.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = model.forward(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            print('loss: ', loss.item())\n",
    "            print('accuracy: ', accuracy(output, label))\n",
    "            print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
