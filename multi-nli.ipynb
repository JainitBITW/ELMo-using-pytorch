{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/turning/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/turning/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import cuda\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the CONSTANTS \n",
    "EXCLUDE_STOPWORDS = True\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 100\n",
    "GLOVE_PATH = 'glove/glove.6B.100d.txt'\n",
    "DEVICE = 'cuda'\n",
    "if cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/home/turning/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|██████████| 3/3 [00:00<00:00, 112.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"multi_nli\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove[line[0]] = torch.tensor([float(x) for x in line[1:]])\n",
    "\n",
    "# create a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "glove['<unk>'] = torch.mean(torch.stack(list(glove.values())), dim=0)\n",
    "glove['<pad>'] = torch.zeros(EMBEDDING_DIM)\n",
    "glove['<start>'] = torch.rand(EMBEDDING_DIM)\n",
    "glove['<end>'] = torch.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the word_2_idx and idx_2_word dictionaries and the embedding matrix\n",
    "word_2_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
    "idx_2_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
    "embedding_matrix = np.zeros((len(glove.values()), EMBEDDING_DIM))\n",
    "embedding_matrix[0] = glove['<pad>']\n",
    "embedding_matrix[1] = glove['<unk>']\n",
    "embedding_matrix[2] = glove['<start>']\n",
    "embedding_matrix[3] = glove['<end>']\n",
    "\n",
    "for i, word in enumerate(glove.keys()):\n",
    "    if word not in word_2_idx:\n",
    "        word_2_idx[word] = len(word_2_idx)\n",
    "        idx_2_word[len(idx_2_word)] = word\n",
    "        embedding_matrix[word_2_idx[word]] = glove[word]\n",
    "\n",
    "# convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {}\n",
    "new_dataset['train'] = dataset['train'][:5]\n",
    "new_dataset['validation'] = dataset['validation_matched'][:5]\n",
    "dataset=new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product and geography are what make cream skimming work. ',\n",
      " 'You lose the things to the following level if the people recall.',\n",
      " 'A member of my team will execute your orders with immense precision.',\n",
      " 'This information belongs to them.',\n",
      " 'The tennis shoes have a range of prices.']\n",
      "['Conceptually cream skimming has two basic dimensions - product and '\n",
      " 'geography.',\n",
      " 'you know during the season and i guess at at your level uh you lose them to '\n",
      " 'the next level if if they decide to recall the the parent team the Braves '\n",
      " 'decide to call to recall a guy from triple A then a double A guy goes up to '\n",
      " 'replace him and a single A guy goes up to replace him',\n",
      " 'One of our number will carry out your instructions minutely.',\n",
      " 'How do you know? All this is their information again.',\n",
      " 'yeah i tell you what though if you go price some of those tennis shoes i can '\n",
      " \"see why now you know they're getting up in the hundred dollar range\"]\n",
      "['This information belongs to them.',\n",
      " 'You lose the things to the following level if the people recall.',\n",
      " 'A member of my team will execute your orders with immense precision.',\n",
      " 'The tennis shoes have a range of prices.',\n",
      " 'Product and geography are what make cream skimming work. ']\n",
      "['How do you know? All this is their information again.',\n",
      " 'you know during the season and i guess at at your level uh you lose them to '\n",
      " 'the next level if if they decide to recall the the parent team the Braves '\n",
      " 'decide to call to recall a guy from triple A then a double A guy goes up to '\n",
      " 'replace him and a single A guy goes up to replace him',\n",
      " 'One of our number will carry out your instructions minutely.',\n",
      " 'yeah i tell you what though if you go price some of those tennis shoes i can '\n",
      " \"see why now you know they're getting up in the hundred dollar range\",\n",
      " 'Conceptually cream skimming has two basic dimensions - product and '\n",
      " 'geography.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['train']['premise'])\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['train']['hypothesis'])\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['train']['label'])\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['validation']['premise'])\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['validation']['hypothesis'])\n",
    "random.seed(42)\n",
    "random.shuffle(new_dataset['validation']['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = {'train': [], 'validation':[]}\n",
    "cat_to_name={'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "#entailment (0), neutral (1), contradiction (2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(sentence, stop_words_remove):\n",
    "    sentence = sentence.split(' ')\n",
    "    if stop_words_remove:\n",
    "        sentence = [word.lower() for word in sentence if word.lower() not in stop_words]\n",
    "    else:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "    sentence = ['<start> '] + sentence+ ['<end>']\n",
    "    sentence = [word_2_idx[word] if word in word_2_idx else word_2_idx['<unk>'] for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# convertng the dataset into list of dicts \n",
    "raw_datasets = {'train': [], 'validation':[]}\n",
    "for i in dataset:\n",
    "    # for j in (range(len(dataset[i]['genre']))):\n",
    "    print(len(dataset[i]))\n",
    "\n",
    "    for j in range(len(dataset[i]['premise'])):\n",
    "\n",
    "        if dataset[i]['label'][j]== -1:\n",
    "            continue\n",
    "       \n",
    "        tokens = preprocessing(dataset[i]['premise'][j], EXCLUDE_STOPWORDS)\n",
    "        tokens_hypothesis = preprocessing(dataset[i]['hypothesis'][j], EXCLUDE_STOPWORDS)\n",
    "        \n",
    "      \n",
    "        \n",
    "        raw_datasets[i].append({'premise': tokens, 'hypothesis': tokens_hypothesis, 'label': dataset[i]['label'][j]})        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pretrain = {'train': [], 'validation':[]}\n",
    "dataset_nli = {'train': [], 'validation':[]}\n",
    "for i in raw_datasets:\n",
    "    for j in raw_datasets[i]:\n",
    "        j['premise'] = torch.LongTensor(j['premise'])\n",
    "        j['hypothesis'] = torch.LongTensor(j['hypothesis'])\n",
    "        j['label'] = torch.LongTensor([j['label']])\n",
    "        dataset_pretrain[i].append({'sentence': j['premise'], 'label': j['premise'][1:]})\n",
    "        dataset_pretrain[i].append({'sentence': j['hypothesis'], 'label': j['hypothesis'][1:]})\n",
    "        dataset_nli[i].append({'sentence': (j['premise'] , j['hypothesis']), 'label': j['label']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 160, 16544, 5079, 1, 113, 1572, 140454, 11637, 4226, 1253, 245, 5079, 53149, 2159, 643, 6066, 33033, 6, 1, 3], [1, 245, 5079, 53149, 3792, 3466, 3854, 9346, 6, 1, 3]), 2)\n"
     ]
    }
   ],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        random.shuffle(data)\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['sentence'], self.data[idx]['label']\n",
    "    \n",
    "pretrain_dataset = {'train': PretrainDataset(dataset_pretrain['train']), 'validation': PretrainDataset(dataset_pretrain['validation'])}\n",
    "nli_dataset = {'train': PretrainDataset(dataset_nli['train']), 'validation': PretrainDataset(dataset_nli['validation']) }\n",
    "\n",
    "print(nli_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    sentences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True)\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return  padded_sentences,padded_labels\n",
    "\n",
    "def custom_collate_nli(batch):\n",
    "    premises, hypothesis = [item[0][0] for item in batch], [item[0][1] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "  \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_premises = torch.nn.utils.rnn.pad_sequence(premises, batch_first=True)\n",
    "    padded_hypothesis = torch.nn.utils.rnn.pad_sequence(hypothesis, batch_first=True)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    return  (padded_premises, padded_hypothesis),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_loaders={}\n",
    "nli_loaders={}\n",
    "for i in pretrain_dataset:\n",
    "    pretrain_loaders[i] = DataLoader(pretrain_dataset[i], batch_size=BATCH_SIZE, collate_fn=custom_collate)\n",
    "    nli_loaders[i] = DataLoader(nli_dataset[i], batch_size=BATCH_SIZE, collate_fn=custom_collate_nli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the model which we are going to pretrain\n",
    "class ELMo(nn.Module):\n",
    "    '''this class implements the ELMo model using the BI-LSTM architecture like by stacking two LSTM layers \n",
    "    the model is just the head and needs body such as linear layer , mlp , etc based on the task  '''\n",
    "    def __init__(self, embedding_dim,  hidden_dim1, hidden_dim2 ,batch_size, num_layers=2):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding= nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.weight1 = nn.Parameter(torch.randn(1))\n",
    "        self.weight2 = nn.Parameter(torch.randn(1))\n",
    "        self.lambda1 = nn.Parameter(torch.randn(1))\n",
    "\n",
    "\n",
    "    def forward(self, input): \n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        input_embeddings = self.embedding(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # passing the embeddings to the first LSTM layer\n",
    "        output1 , (hidden1, cell1) = self.lstm1(input_embeddings) # [batch_size, seq_len, hidden_dim1]\n",
    "\n",
    "        # passing the output of the first LSTM layer to the second LSTM layer\n",
    "        output2 , (hidden2, cell2) = self.lstm2(output1) # [batch_size, seq_len, hidden_dim2]\n",
    "        # adding the two outputs of the LSTM layers\n",
    "        \n",
    "        weighted_output = self.lambda1*((self.weight1 * output1) +( self.weight2 * output2))\n",
    "\n",
    "        return weighted_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_model(nn.Module):\n",
    "    '''this class implements the language model using the ELMo model as the head and a linear layer as the body'''\n",
    "    def __init__(self, Elmo_model, vocab_size, embedding_dim):\n",
    "        super(Language_model, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.elmo = Elmo_model\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.vocab_size)\n",
    "    def forward(self, input):\n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        elmo_output = self.elmo(input) # [batch_size, seq_len, embedding_dim]\n",
    "        output = self.linear(elmo_output) # [batch_size, seq_len, vocab_size]\n",
    "        output = F.log_softmax(output, dim=2).permute(0,2,1)[:,:,:-1] # [batch_size, vocab_size, seq_len-1]\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI(nn.Module): \n",
    "\n",
    "    def __init__(self, Elmo_model, embedding_dim, num_classes=3):\n",
    "        super(NLI, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo = Elmo_model\n",
    "        self.linear = nn.Linear(self.embedding_dim*2,25)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(25, self.num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input = [batch_size,(premise, hypothesis)]\n",
    "        premise = input[:,0] # [batch_size, seq_len]\n",
    "        hypothesis = input[:,1] # [batch_size, seq_len]\n",
    "        # getting the imput embeddings\n",
    "        elmo_output_premise = self.elmo(premise) # [batch_size, seq_len, embedding_dim]\n",
    "        elmo_output_hypothesis = self.elmo(hypothesis) # [batch_size, seq_len, embedding_dim]\n",
    "        sentence_embeddings_premise = []\n",
    "        sentence_embeddings_hypothesis = []\n",
    "        for i in range(len(elmo_output_premise)):\n",
    "            sentence_embeddings_premise.append(torch.mean(elmo_output_premise[i], dim=0))\n",
    "        for i in range(len(elmo_output_hypothesis)):\n",
    "            sentence_embeddings_hypothesis.append(torch.mean(elmo_output_hypothesis[i], dim=0))\n",
    "        sentence_embeddings_input = sentence_embeddings_premise + sentence_embeddings_hypothesis\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings_input)  # convert list to tensor\n",
    "        output1 = self.linear(sentence_embeddings) # [batch_size, num_classes]\n",
    "        output1 = self.dropout(output1)\n",
    "        output = self.linear2(output1)\n",
    "        output = F.log_softmax(output, dim=1) # [batch_size, num_classes]\n",
    "\n",
    "        return output\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
